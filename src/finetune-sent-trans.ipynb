{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import psycopg2\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from psycopg2 import sql\n",
    "database = {\n",
    "    \"database\": \"postgres\",\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"password\",\n",
    "    \"host\": \"192.168.1.16\",\n",
    "    \"port\": \"5432\"\n",
    "}\n",
    "\n",
    "def connect_to_db():\n",
    "\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            dbname=database[\"database\"],\n",
    "            user=database[\"user\"],\n",
    "            password=database[\"password\"],\n",
    "            host=database[\"host\"],\n",
    "            port=database[\"port\"]\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        raise e\n",
    "\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = connect_to_db()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers.optimization import get_scheduler\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import ast\n",
    "import urllib\n",
    "model_name = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentDataset(Dataset):\n",
    "    @staticmethod\n",
    "    def _randomly_sample_docs(amount: int):\n",
    "        conn = connect_to_db()\n",
    "        \n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"SET search_path TO ag_catalog;\")\n",
    "        cursor.execute(\n",
    "            f\"\"\"\n",
    "            SELECT * FROM (\n",
    "                    SELECT * FROM cypher('gnd', $$\n",
    "                    MATCH (d:Document)\n",
    "                    RETURN d\n",
    "                $$) AS (d agtype)\n",
    "            ) AS subquery\n",
    "            ORDER BY random()\n",
    "            LIMIT {amount};\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "        documents = [json.loads(a[0].replace(\"::vertex\", \"\")) for a in cursor.fetchall()]\n",
    "        \n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        print(\"Documents fetched!\")\n",
    "        return documents\n",
    "\n",
    "    @staticmethod\n",
    "    def _grab_subjects(documents: list):\n",
    "        conn = connect_to_db()\n",
    "\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"SET search_path TO ag_catalog;\")\n",
    "        cursor.execute(\"CREATE INDEX subj_id ON gnd.\\\"Subject\\\" USING gin (properties);\")\n",
    "        subjects = []\n",
    "        for document in tqdm(documents, desc=\"Loading subjects...\"):\n",
    "            cursor.execute(f\"\"\"\n",
    "                SELECT * FROM cypher('gnd', $$\n",
    "                    MATCH (d:Document)-[:DOC_SUBJECT]->(s:Subject)\n",
    "                    WHERE d.id = '{document[\"properties\"][\"id\"]}'\n",
    "                    RETURN s\n",
    "                $$) AS (s agtype);\n",
    "            \"\"\")\n",
    "            subj = cursor.fetchall()\n",
    "            doc_subjects = [json.loads(a[0].replace(\"::vertex\", \"\")) for a in subj]\n",
    "\n",
    "            subjects.append(doc_subjects)\n",
    "        cursor.execute(\"DROP INDEX IF EXISTS gnd.\\\"subj_id\\\";\")\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "        return subjects\n",
    "\n",
    "    # @staticmethod\n",
    "    # def _grab_embeddings(subject_list: list[list[str]]):\n",
    "        \n",
    "    #     conn = connect_to_db()\n",
    "    #     cursor = conn.cursor()\n",
    "    #     cursor.execute(\"CREATE INDEX label_indx ON label_embeddings (label_code)\")\n",
    "    #     embeddings = []\n",
    "    #     for subset in tqdm(subject_list, desc=\"Loading embeddings...\"):\n",
    "    #         embeddings_subset = []\n",
    "    #         for subject in subset:\n",
    "    #             sub_code = subject[\"properties\"][\"code\"]\n",
    "    #             cursor.execute(\"SELECT embedding FROM label_embeddings WHERE label_code = %s\", (sub_code,))\n",
    "    #             vector_string = cursor.fetchone()[0]\n",
    "    #             vector = torch.tensor(ast.literal_eval(vector_string), dtype=torch.float32)\n",
    "    #             embeddings_subset.append(vector)\n",
    "            \n",
    "    #         embeddings.append(embeddings_subset)\n",
    "        \n",
    "    #     cursor.execute(\"DROP INDEX IF EXISTS label_indx;\")\n",
    "    #     cursor.close()\n",
    "    #     conn.close()\n",
    "    #     return embeddings\n",
    "\n",
    "    @staticmethod\n",
    "    def _grab_embeddings(subject_list: list[list[str]]):\n",
    "        conn = connect_to_db()\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Create index for faster lookups\n",
    "        cursor.execute(\"CREATE INDEX label_indx ON label_embeddings (label_code)\")\n",
    "        \n",
    "        embeddings = []\n",
    "        for subset in tqdm(subject_list, desc=\"Loading embeddings...\"):\n",
    "        # for subset in subject_list:\n",
    "            embeddings_subset = []\n",
    "            for subject in subset:\n",
    "                \n",
    "                sub_code = urllib.parse.unquote(subject[\"properties\"][\"code\"])\n",
    "\n",
    "                # Fetch the vector from the database\n",
    "                cursor.execute(\"SELECT embedding FROM label_embeddings WHERE label_code = %s;\", (sub_code,))\n",
    "                result = cursor.fetchone()\n",
    "                \n",
    "                if result is None:\n",
    "                    # Handle missing label_code\n",
    "                    print(f\"Warning: No embedding found for label_code {sub_code}\")\n",
    "                    continue\n",
    "                \n",
    "                embedding_data = result[0]\n",
    "                \n",
    "                if embedding_data is None:\n",
    "                    # Handle NULL embeddings\n",
    "                    print(f\"Warning: NULL embedding for label_code {sub_code}\")\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    # Convert to PyTorch tensor\n",
    "                    if isinstance(embedding_data, list):  # If already a list-like object\n",
    "                        vector = torch.tensor(embedding_data, dtype=torch.float32)\n",
    "                    elif isinstance(embedding_data, str):  # If it's a string like \"[0.1, 0.2, 0.3]\"\n",
    "                        vector = torch.tensor(ast.literal_eval(embedding_data), dtype=torch.float32)\n",
    "                    else:\n",
    "                        raise ValueError(f\"Unexpected embedding format for label_code {sub_code}: {embedding_data}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing embedding for label_code {sub_code}: {e}\")\n",
    "                    continue\n",
    "                \n",
    "                embeddings_subset.append(vector)\n",
    "            \n",
    "            embeddings.append(torch.stack(embeddings_subset).mean(dim=0))\n",
    "        \n",
    "        # Drop the index after processing\n",
    "        cursor.execute(\"DROP INDEX IF EXISTS label_indx;\")\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "    def __init__(self, size):\n",
    "        self._documents = self._randomly_sample_docs(size)\n",
    "        self._subjects = self._grab_subjects(self._documents)\n",
    "        self._embeddings = self._grab_embeddings(self._subjects)\n",
    "        self._len = size\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"document\": self._documents[idx],\n",
    "            \"label_embeddings\": self._embeddings[idx]\n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomDocumentDataset(Dataset):\n",
    "    @staticmethod\n",
    "    def _randomly_sample_docs(amount: int):\n",
    "        conn = connect_to_db()\n",
    "        \n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"SET search_path TO ag_catalog;\")\n",
    "        cursor.execute(\n",
    "            f\"\"\"\n",
    "            SELECT * FROM (\n",
    "                    SELECT * FROM cypher('gnd', $$\n",
    "                    MATCH (d:Document)\n",
    "                    RETURN d\n",
    "                $$) AS (d agtype)\n",
    "            ) AS subquery\n",
    "            ORDER BY random()\n",
    "            LIMIT {amount};\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "        documents = [json.loads(a[0].replace(\"::vertex\", \"\")) for a in cursor.fetchall()]\n",
    "        \n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        print(\"Documents fetched!\")\n",
    "        return documents\n",
    "\n",
    "    @staticmethod\n",
    "    def _grab_subjects(documents: list):\n",
    "        conn = connect_to_db()\n",
    "\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"SET search_path TO ag_catalog;\")\n",
    "        cursor.execute(\"CREATE INDEX subj_id ON gnd.\\\"Subject\\\" USING gin (properties);\")\n",
    "        subjects = []\n",
    "        for document in tqdm(documents, desc=\"Loading subjects...\"):\n",
    "            cursor.execute(f\"\"\"\n",
    "                SELECT * FROM cypher('gnd', $$\n",
    "                    MATCH (d:Document)-[:DOC_SUBJECT]->(s:Subject)\n",
    "                    WHERE d.id = '{document[\"properties\"][\"id\"]}'\n",
    "                    RETURN s\n",
    "                $$) AS (s agtype);\n",
    "            \"\"\")\n",
    "            subj = cursor.fetchall()\n",
    "            doc_subjects = [json.loads(a[0].replace(\"::vertex\", \"\")) for a in subj]\n",
    "\n",
    "            subjects.append(doc_subjects)\n",
    "        cursor.execute(\"DROP INDEX IF EXISTS gnd.\\\"subj_id\\\";\")\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "        return subjects\n",
    "\n",
    "    def _sample_one_doc(self):\n",
    "        document = self._randomly_sample_docs(1)\n",
    "        subjects = self._grab_subjects(document)\n",
    "        embeddings = self._grab_embeddings(subjects)\n",
    "\n",
    "        return (document, subjects, embeddings)\n",
    "\n",
    "    @staticmethod\n",
    "    def _grab_embeddings(subject_list: list[list[str]]):\n",
    "        conn = connect_to_db()\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Create index for faster lookups\n",
    "        cursor.execute(\"CREATE INDEX label_indx ON label_embeddings (label_code)\")\n",
    "        \n",
    "        embeddings = []\n",
    "        for subset in tqdm(subject_list, desc=\"Loading embeddings...\"):\n",
    "        # for subset in subject_list:\n",
    "            embeddings_subset = []\n",
    "            for subject in subset:\n",
    "                \n",
    "                sub_code = urllib.parse.unquote(subject[\"properties\"][\"code\"])\n",
    "\n",
    "                # Fetch the vector from the database\n",
    "                cursor.execute(\"SELECT embedding FROM label_embeddings WHERE label_code = %s;\", (sub_code,))\n",
    "                result = cursor.fetchone()\n",
    "                \n",
    "                if result is None:\n",
    "                    # Handle missing label_code\n",
    "                    print(f\"Warning: No embedding found for label_code {sub_code}\")\n",
    "                    continue\n",
    "                \n",
    "                embedding_data = result[0]\n",
    "                \n",
    "                if embedding_data is None:\n",
    "                    # Handle NULL embeddings\n",
    "                    print(f\"Warning: NULL embedding for label_code {sub_code}\")\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    # Convert to PyTorch tensor\n",
    "                    if isinstance(embedding_data, list):  # If already a list-like object\n",
    "                        vector = torch.tensor(embedding_data, dtype=torch.float32)\n",
    "                    elif isinstance(embedding_data, str):  # If it's a string like \"[0.1, 0.2, 0.3]\"\n",
    "                        vector = torch.tensor(ast.literal_eval(embedding_data), dtype=torch.float32)\n",
    "                    else:\n",
    "                        raise ValueError(f\"Unexpected embedding format for label_code {sub_code}: {embedding_data}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing embedding for label_code {sub_code}: {e}\")\n",
    "                    continue\n",
    "                \n",
    "                embeddings_subset.append(vector)\n",
    "            \n",
    "            embeddings.append(torch.stack(embeddings_subset).mean(dim=0))\n",
    "        \n",
    "        # Drop the index after processing\n",
    "        cursor.execute(\"DROP INDEX IF EXISTS label_indx;\")\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "    def __init__(self, size):\n",
    "        self._len = size\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        document, subjects, embeddings = self._sample_one_doc()\n",
    "        return {\n",
    "            \"document\": document[0],\n",
    "            \"label_embeddings\": embeddings[0]\n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents fetched!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading subjects...: 100%|██████████| 1/1 [00:00<00:00,  8.45it/s]\n"
     ]
    }
   ],
   "source": [
    "test_dataset = RandomDocumentDataset(1)\n",
    "print(test_dataset[0][\"label_embeddings\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineTunedModel(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super(FineTunedModel, self).__init__()\n",
    "        self.base_model = base_model\n",
    "\n",
    "    def _mean_pooling(self, model_output, attention_mask):\n",
    "        token_embeddings = model_output[0] \n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        model_output = self.base_model(input_ids = input_ids, attention_mask=attention_mask)\n",
    "        sentence_embeddings = self._mean_pooling(model_output, attention_mask)\n",
    "\n",
    "        return sentence_embeddings\n",
    "\n",
    "embedding_model = FineTunedModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_loss = nn.CosineEmbeddingLoss()\n",
    "\n",
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(embedding_model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents fetched!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading subjects...: 100%|██████████| 1000/1000 [01:46<00:00,  9.39it/s]\n",
      "Loading embeddings...: 100%|██████████| 1000/1000 [00:30<00:00, 32.70it/s]\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "dataset = DocumentDataset(size=1000)\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    documents = [item[\"document\"] for item in batch]\n",
    "    label_embeddings = [item[\"label_embeddings\"] for item in batch]\n",
    "    \n",
    "    return {\n",
    "        \"documents\": documents,\n",
    "        \"label_embeddings\": label_embeddings,\n",
    "    }\n",
    "\n",
    "batch_size = 16\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [09:56<1:29:28, 596.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Avg Loss: 0.2452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [19:49<1:19:14, 594.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Avg Loss: 0.1783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [30:02<1:10:20, 602.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Avg Loss: 0.1539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [40:08<1:00:24, 604.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Avg Loss: 0.1375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [50:02<50:02, 600.44s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Avg Loss: 0.1232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [1:00:08<40:09, 602.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Avg Loss: 0.1109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [1:09:50<29:47, 595.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Avg Loss: 0.1019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [1:19:34<19:44, 592.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Avg Loss: 0.0940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [1:29:27<09:52, 592.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Avg Loss: 0.0873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [1:39:24<00:00, 596.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Avg Loss: 0.0811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import unquote\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "embedding_model.to(device)\n",
    "\n",
    "epochs = 5\n",
    "embedding_model.train()\n",
    "for i in tqdm(range(epochs)):\n",
    "    epoch_loss = 0\n",
    "    for batch in data_loader:\n",
    "        loss=0\n",
    "        documents = batch[\"documents\"]\n",
    "        label_embeddings = batch[\"label_embeddings\"]\n",
    "        input_text = [unquote(doc[\"properties\"][\"title\"]) + \" \" + unquote( doc[\"properties\"][\"content\"]) for doc in documents]\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        input_ids = inputs[\"input_ids\"].to(device)\n",
    "        attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "        doc_embeddings = embedding_model(input_ids, attention_mask)\n",
    "        \n",
    "        for i , (doc_embedding, label_embedding) in enumerate(zip(doc_embeddings, label_embeddings)):\n",
    "            avg_embedding = label_embedding.to(device)\n",
    "\n",
    "            target = torch.tensor(1.0, device=device)\n",
    "            loss += cosine_loss(doc_embedding, avg_embedding, target)\n",
    "        \n",
    "        loss /= len(doc_embeddings)\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {i + 1}/{epochs}, Avg Loss: {epoch_loss/(len(dataset)/batch_size):.4f}\")\n",
    "\n",
    "torch.save(embedding_model, \"fine_tuned_model_complete.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fii-advrn-project-h28zvCYG-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
